{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILNp15yRxrNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "025ece4c-db5a-4d70-ad4d-9a4d91885f60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSVuYPJ9xy9g",
        "outputId": "ee921c2b-03bc-4a34-9c4b-96f0e19248d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan  7 13:06:40 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0              47W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation"
      ],
      "metadata": {
        "id": "aoMCltNAU1tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Data Augmentation\n",
        "\"\"\"\n",
        "\n",
        "from imgaug import augmenters as iaa\n",
        "import numpy as np\n",
        "import cv2\n",
        "import json\n",
        "import glob as gb\n",
        "\n",
        "#--- a function for image augmentation ---\n",
        "def data_aug(img, flags1, flags2):\n",
        "\n",
        "  img_shape = img.shape\n",
        "  images = np.random.randint(0, 1, (1, img_shape[0], img_shape[1], img_shape[2]), dtype=np.uint8)\n",
        "  images[0] = img\n",
        "  seq1 = iaa.Sequential([\n",
        "      iaa.Sometimes(flags1[0], iaa.GaussianBlur(sigma=(2, 11))),\n",
        "\n",
        "      iaa.Sometimes(flags1[1], iaa.MedianBlur(k=(3, 25))),\n",
        "\n",
        "      iaa.Sometimes(flags1[2], iaa.Add((-10, 10))),\n",
        "\n",
        "      iaa.Sometimes(flags1[3], iaa.WithColorspace(to_colorspace=\"HSV\",\n",
        "                                                  children=iaa.WithChannels(channels=[1],\n",
        "                                                                            children=iaa.Add((-5, 5))))),\n",
        "\n",
        "      iaa.Sometimes(flags1[4], iaa.WithColorspace(to_colorspace=\"HSV\",\n",
        "                                                  children=iaa.WithChannels(channels=[2],\n",
        "                                                                            children=iaa.Add((-5, 5))))),\n",
        "\n",
        "      iaa.Sometimes(flags1[5], iaa.Multiply((0.9, 1.1))),\n",
        "\n",
        "      iaa.Sometimes(flags1[6], iaa.Sharpen(alpha=(0, 1), lightness=(0.75, 1.25))),\n",
        "\n",
        "      iaa.Sometimes(flags1[7], iaa.AdditiveGaussianNoise(loc=0, scale=(0, 5))),\n",
        "\n",
        "      iaa.Sometimes(flags1[8], iaa.SaltAndPepper(p=(0, 0.005))),\n",
        "\n",
        "      #iaa.Sometimes(flags1[9], iaa.ContrastNormalization((0.85, 1.15))),\n",
        "\n",
        "      iaa.Sometimes(flags1[10], iaa.WithColorspace(to_colorspace=\"HSV\",\n",
        "                                                    children=iaa.WithChannels(channels=[1, 2],\n",
        "                                                                              children=iaa.Add((-5, 5))))),\n",
        "      iaa.Sometimes(0, iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.15)),\n",
        "      iaa.Sometimes(0, iaa.Emboss(alpha=(0, 1), strength=(0, 4))),\n",
        "      iaa.Sometimes(flags1[13], iaa.GaussianBlur(sigma=(4, 11))),\n",
        "      iaa.Sometimes(flags2[0], iaa.Fliplr(1)),\n",
        "      iaa.Sometimes(flags2[1], iaa.Flipud(1)),\n",
        "      iaa.Sometimes(flags2[2], iaa.Affine(rotate=(-90, 90) , mode= 'edge')),\n",
        "      iaa.Sometimes(flags2[3], iaa.Affine(scale=(0.8, 1.2) , mode='edge'))\n",
        "\n",
        "  ])\n",
        "  images_aug = seq1.augment_images(images)\n",
        "  img_aug1 = images_aug[0]\n",
        "  return img_aug1\n",
        "\n",
        "# ---this function manage number of Augmenations for each class---\n",
        "def manage_aug(imgs, n_fold):\n",
        "\n",
        "\n",
        "  flag1 = [0] * 14\n",
        "  flag2 = [0] * 4\n",
        "  imgs_aug = []\n",
        "  for img in imgs:\n",
        "\n",
        "    # create n-fold random numbers between 1 to 15 without repeat\n",
        "    imgs_aug.append(img)\n",
        "    random_Nums = []\n",
        "    len_rand_Nums = 1\n",
        "    while len_rand_Nums < n_fold:\n",
        "      temp = np.random.randint(1 , 16)\n",
        "      if temp not in random_Nums :\n",
        "        random_Nums.append(temp)\n",
        "        len_rand_Nums+=1\n",
        "\n",
        "    # This Section aim to augment the image\n",
        "    for j in range(len(random_Nums)) :\n",
        "      num = random_Nums[j]\n",
        "\n",
        "      num_bin = bin(num).replace('0b' , '') # converting number_aug from decimal to binary to serve flag2\n",
        "\n",
        "      # fill num_bin by zero because its length must be 4\n",
        "      if len(num_bin) == 1 :\n",
        "        num_bin = '000' + num_bin\n",
        "      elif len(num_bin) == 2 :\n",
        "        num_bin = '00' + num_bin\n",
        "      elif len(num_bin) == 3 :\n",
        "        num_bin = '0' + num_bin\n",
        "\n",
        "      flag2[0] , flag2[1] , flag2[2] , flag2[3] = int(num_bin[3]) , int(num_bin[2])  , int(num_bin[1]) , int(num_bin[0])\n",
        "\n",
        "      img_aug = data_aug(img, flags1 = flag1 , flags2= flag2)\n",
        "      imgs_aug.append(img_aug)\n",
        "  return imgs_aug\n",
        "\n"
      ],
      "metadata": {
        "id": "9E5SIfP3x3iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing vit-keras and tensorflow_addons libraries"
      ],
      "metadata": {
        "id": "12H4UcswVFDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow_addons\n",
        "! pip install vit-keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oarwsbOi99iR",
        "outputId": "367ed380-745f-47af-e4b9-f9b5e4d0d335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (2.13.3)\n",
            "Requirement already satisfied: vit-keras in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from vit-keras) (1.11.4)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.10/dist-packages (from vit-keras) (0.22.0)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->vit-keras) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep learning models"
      ],
      "metadata": {
        "id": "8453Ck6xVNFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"\n",
        "Deep Learning Models\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input as resnet_prep\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input as mob_prep\n",
        "from tensorflow.keras.applications.densenet import DenseNet121, preprocess_input as dens_prep\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_prep\n",
        "from vit_keras import vit, utils as vit_utils\n",
        "from vit_keras.vit import preprocess_inputs as vit_prep\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "\n",
        "\n",
        "\n",
        "def vgg16(input_shape=(224, 224, 3), base_trainable=False, weights=\"imagenet\", optimizer='adam'):\n",
        "\n",
        "  ## Loading VGG16 model\n",
        "  base_model = VGG16(weights=weights, include_top=False, input_shape=input_shape)\n",
        "  base_model.trainable = True ## Not trainable weights\n",
        "  # for layer in base_model.layers[:-6]:\n",
        "  #  layer.trainable = False\n",
        "\n",
        "  data_augmentation = models.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "  ])\n",
        "\n",
        "  flatten_layer = layers.Flatten()\n",
        "  dense_layer_1 = layers.Dense(1024, activation='relu')\n",
        "  # dense_layer_2 = layers.Dense(512, activation='relu')\n",
        "  prediction_layer = layers.Dense(3, activation='softmax')\n",
        "\n",
        "  model = models.Sequential([\n",
        "      layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "      layers.RandomRotation(0.2),\n",
        "      base_model,\n",
        "      flatten_layer,\n",
        "      # dense_layer_1,\n",
        "      #dense_layer_2,\n",
        "      prediction_layer\n",
        "  ])\n",
        "\n",
        "\n",
        "  # model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  model.compile(optimizer=Adam(learning_rate=0.001), loss=CategoricalCrossentropy() , metrics=['accuracy'])\n",
        "  # model.compile(optimizer=Adam(lr=0.001), loss=SparseCategoricalCrossentropy(from_logits=True) , metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "\n",
        "def resnet50(input_shape=(224, 224, 3), base_trainable=False, weights=\"imagenet\", optimizer='adam'):\n",
        "\n",
        "  ## Loading VGG16 model\n",
        "  base_model = ResNet50(weights=weights, include_top=False, input_shape=input_shape)\n",
        "  base_model.trainable = True ## Not trainable weights\n",
        "  # for layer in base_model.layers[:-6]:\n",
        "  #  layer.trainable = False\n",
        "\n",
        "  data_augmentation = models.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "  ])\n",
        "\n",
        "  flatten_layer = layers.Flatten()\n",
        "  dense_layer_1 = layers.Dense(1024, activation='relu')\n",
        "  # dense_layer_2 = layers.Dense(512, activation='relu')\n",
        "  prediction_layer = layers.Dense(3, activation='softmax')\n",
        "\n",
        "  model = models.Sequential([\n",
        "      layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "      layers.RandomRotation(0.2),\n",
        "      base_model,\n",
        "      flatten_layer,\n",
        "      # dense_layer_1,\n",
        "      #dense_layer_2,\n",
        "      prediction_layer\n",
        "  ])\n",
        "\n",
        "\n",
        "  # model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  model.compile(optimizer=Adam(learning_rate=0.001), loss=CategoricalCrossentropy() , metrics=['accuracy'])\n",
        "  # model.compile(optimizer=Adam(lr=0.001), loss=SparseCategoricalCrossentropy(from_logits=True) , metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "\n",
        "def mobnetv2(input_shape=(224, 224, 3), base_trainable=False, weights=\"imagenet\", optimizer='adam'):\n",
        "\n",
        "  ## Loading VGG16 model\n",
        "  base_model = MobileNetV2(weights=weights, include_top=False, input_shape=input_shape)\n",
        "  base_model.trainable = True ## Not trainable weights\n",
        "  # for layer in base_model.layers[:-6]:\n",
        "  #  layer.trainable = False\n",
        "\n",
        "  data_augmentation = models.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "  ])\n",
        "\n",
        "  flatten_layer = layers.Flatten()\n",
        "  dense_layer_1 = layers.Dense(1024, activation='relu')\n",
        "  # dense_layer_2 = layers.Dense(512, activation='relu')\n",
        "  prediction_layer = layers.Dense(3, activation='softmax')\n",
        "\n",
        "  model = models.Sequential([\n",
        "      layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "      layers.RandomRotation(0.2),\n",
        "      base_model,\n",
        "      flatten_layer,\n",
        "      # dense_layer_1,\n",
        "      #dense_layer_2,\n",
        "      prediction_layer\n",
        "  ])\n",
        "\n",
        "\n",
        "  # model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  model.compile(optimizer=Adam(learning_rate=0.001), loss=CategoricalCrossentropy() , metrics=['accuracy'])\n",
        "  # model.compile(optimizer=Adam(lr=0.001), loss=SparseCategoricalCrossentropy(from_logits=True) , metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def densenet121(input_shape=(224, 224, 3), base_trainable=False, weights=\"imagenet\", optimizer='adam'):\n",
        "  base_model = tf.keras.applications.densenet.DenseNet121(weights=weights, include_top=False, input_shape=input_shape)\n",
        "  base_model.trainable = True ## Not trainable weights\n",
        "  # for layer in base_model.layers[:-6]:\n",
        "  #  layer.trainable = False\n",
        "\n",
        "  data_augmentation = models.Sequential([\n",
        "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "  ])\n",
        "\n",
        "  flatten_layer = layers.Flatten()\n",
        "  dense_layer_1 = layers.Dense(1024, activation='relu')\n",
        "  # dense_layer_2 = layers.Dense(512, activation='relu')\n",
        "  prediction_layer = layers.Dense(3, activation='softmax')\n",
        "\n",
        "  model = models.Sequential([\n",
        "      layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "      layers.RandomRotation(0.2),\n",
        "      base_model,\n",
        "      flatten_layer,\n",
        "      # dense_layer_1,\n",
        "      #dense_layer_2,\n",
        "      prediction_layer\n",
        "  ])\n",
        "\n",
        "\n",
        "  # model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  model.compile(optimizer=Adam(learning_rate=0.001), loss=CategoricalCrossentropy() , metrics=['accuracy'])\n",
        "  # model.compile(optimizer=Adam(lr=0.001), loss=SparseCategoricalCrossentropy(from_logits=True) , metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "def vit_b16(input_shape=(224, 224, 3), base_trainable=False, weights=\"imagenet\", optimizer='adam'):\n",
        "  img_size = 224\n",
        "  base_model = vit.vit_b16(\n",
        "    image_size=(224, 224),\n",
        "    activation='sigmoid',\n",
        "    pretrained=True,\n",
        "    include_top=False,\n",
        "    pretrained_top=False)\n",
        "\n",
        "  model = models.Sequential([\n",
        "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    layers.RandomRotation(0.2),\n",
        "    base_model,\n",
        "    layers.Dense(units=3, activation='softmax')])\n",
        "  model.compile(optimizer=Adam(lr=0.001), loss=CategoricalCrossentropy() , metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def model_selector(model_name):\n",
        "  if model_name=='vgg16':\n",
        "    my_model = vgg16(input_shape=(224, 224, 3), base_trainable=True, weights=\"imagenet\", optimizer='adam')\n",
        "    pre_process = vgg_prep\n",
        "  elif model_name=='resnet50':\n",
        "    my_model = resnet50(input_shape=(224, 224, 3), base_trainable=True, weights=\"imagenet\", optimizer='adam')\n",
        "    pre_process = resnet_prep\n",
        "  elif model_name == 'mobnetv2':\n",
        "    my_model = mobnetv2(input_shape=(224, 224, 3), base_trainable=True, weights=\"imagenet\", optimizer='adam')\n",
        "    pre_process = mob_prep\n",
        "  elif model_name=='densenet121':\n",
        "    my_model = densenet121(input_shape=(224, 224, 3), base_trainable=True, weights=\"imagenet\", optimizer='adam')\n",
        "    pre_process = dens_prep\n",
        "  elif model_name=='vit':\n",
        "    my_model =  vit_b16()\n",
        "    pre_process = vit_prep\n",
        "\n",
        "  return my_model, pre_process\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t5i6y1p1x4f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data from memory"
      ],
      "metadata": {
        "id": "96KzoecKVXtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import glob\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/My Drive/Fungal-Fungal-interaction/data/'\n",
        "code_dir = '/content/drive/My Drive/Fungal-Fungal-interaction/'\n",
        "well_dir = '/content/drive/My Drive/Fungal-Fungal-interaction/data/wells/'\n",
        "model_dir = '/content/drive/My Drive/Fungal-Fungal-interaction/models/'\n",
        "data = pd.read_csv(data_dir + '01_well_data.csv')\n",
        "raw_paths, lbls = data['img_paths'], data['labels']\n",
        "name_lbls = {}\n",
        "x, y = [], []\n",
        "for i in range(len(raw_paths)):\n",
        "  img_name = raw_paths[i].split('/')[-2] + '/' + raw_paths[i].split('/')[-1]\n",
        "  lbl = int(lbls[i])\n",
        "  # img = cv2.imread(well_dir+img_name)\n",
        "  # img = cv2.resize(img, dsize=(224, 224))\n",
        "  img = np.array(load_img(well_dir+img_name, target_size=(224, 224)))\n",
        "  x.append(img), y.append(lbl)\n",
        "x, y = np.array(x), np.array(y)\n",
        "np.random.seed(42)\n",
        "x = np.random.permutation(x)\n",
        "np.random.seed(42)\n",
        "y = np.random.permutation(y)"
      ],
      "metadata": {
        "id": "QW7OSAjFx9-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import glob\n",
        "#pathes = glob.glob('/content/drive/My Drive/Fungal-Fungal-interaction/data/wells/*/')\n",
        "#print(pathes)\n",
        "import os\n",
        "plate_names = [well for well in os.listdir(well_dir)]\n",
        "np.random.seed(42)\n",
        "plate_names = np.random.permutation(plate_names)"
      ],
      "metadata": {
        "id": "yLBWeNIIyBb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and evaluation of the models through 5-fold cross-validation\n",
        "In this block, all models are trained and evaluated, then, the trained models are saved in memory. Also, the performance of each model across five folds are saved in the memory as a dictionary (*.joblib)."
      ],
      "metadata": {
        "id": "HsHopZOaVf-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_names = ['vgg16', 'resnet50', 'mobnetv2', 'densenet121', 'vit']\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_prep\n",
        "from sklearn.metrics import recall_score as recall, precision_score as precision, f1_score\n",
        "\n",
        "k = 5\n",
        "slide = len(x)//k\n",
        "prec, rec, f1, acc = [], [], [], []\n",
        "performance = {}\n",
        "for model_name in model_names:\n",
        "  try:\n",
        "    del my_model\n",
        "  except:\n",
        "    pass\n",
        "  perf_kfold = {}\n",
        "  for i in range(0, k):\n",
        "    print(model_name, \"fold\", i, '\\n', '#'*50)\n",
        "    # Specifying what images for test and what images for training and validation\n",
        "    if i == k-1:\n",
        "      test_idx = np.arange(i*slide, len(x))\n",
        "      rest_idx = np.arange(0, i*slide)\n",
        "    else:\n",
        "      test_idx = np.arange(i*slide, (i+1)*slide)\n",
        "      rest_idx = [id for id in range(len(x)) if id not in test_idx]\n",
        "    x_test, y_test = x[test_idx], y[test_idx]\n",
        "    x_rest, y_rest = x[rest_idx], y[rest_idx]\n",
        "\n",
        "    # Augmentation\n",
        "    num_cls0, num_cls1, num_cls2 = y_rest.tolist().count(0), y_rest.tolist().count(1), y_rest.tolist().count(2)\n",
        "    cls_max = np.max([num_cls0, num_cls1, num_cls2])\n",
        "    n_fold0, n_fold1, n_fold2 = (np.round(cls_max/num_cls0))*2, (np.round(cls_max/num_cls1))*2, (np.round(cls_max/num_cls2))*2\n",
        "    #print(num_cls0, num_cls1, num_cls2, cls_max, n_fold0, n_fold1, n_fold2)\n",
        "    #break\n",
        "    #print(y_rest.shape, x_rest.shape)\n",
        "    #print(type(y_rest))\n",
        "    x_rest_aug, y_rest_aug = [], []\n",
        "    for cls in [0, 1, 2]:\n",
        "      num_cls = y_rest.tolist().count(cls)\n",
        "      #print(num_cls)\n",
        "      if cls!=2:\n",
        "        n_fold = int((np.round(cls_max/num_cls))*2)\n",
        "      else:\n",
        "        n_fold = 14\n",
        "      indexes = np.argwhere(y_rest==cls).reshape(-1)\n",
        "      #print(indexes.shape)\n",
        "      imgs = x_rest[indexes]\n",
        "      img_augs = manage_aug(imgs, n_fold)\n",
        "      img_augs = manage_aug(img_augs, 2)\n",
        "      x_rest_aug.extend(img_augs)\n",
        "      lbls = [cls]*len(img_augs)\n",
        "      y_rest_aug.extend(lbls)\n",
        "\n",
        "    np.random.seed(42)\n",
        "    x_rest = np.random.permutation(x_rest_aug)\n",
        "    np.random.seed(42)\n",
        "    y_rest = np.random.permutation(y_rest_aug)\n",
        "\n",
        "    x_train, y_train = x_rest[:int(0.8*len(y_rest))], y_rest[:int(0.8*len(y_rest))]\n",
        "    x_val, y_val = x_rest[int(0.8*len(y_rest)):], y_rest[int(0.8*len(y_rest)):]\n",
        "\n",
        "    try:\n",
        "      del my_model\n",
        "    except:\n",
        "      pass\n",
        "    my_model, preprocess = model_selector(model_name)\n",
        "    x_train = preprocess(x_train)\n",
        "    x_val = preprocess(x_val)\n",
        "    x_test = preprocess(x_test)\n",
        "    y_train_cat = to_categorical(y_train, num_classes=3)\n",
        "    y_val_cat = to_categorical(y_val, num_classes=3)\n",
        "    y_test_cat = to_categorical(y_test, num_classes=3)\n",
        "\n",
        "    my_model.build((None, 224, 224, 3))\n",
        "    my_model.summary()\n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', patience=20,  restore_best_weights=True)\n",
        "    history = my_model.fit(x_train, y_train_cat, epochs=200, validation_data=(x_val, y_val_cat), batch_size=16, callbacks=[es], verbose=True)\n",
        "\n",
        "    res = {}\n",
        "    train_loss, train_acc, val_loss, val_acc = history.history['loss'], history.history['accuracy'], history.history['val_loss'], history.history['val_accuracy']\n",
        "    res['training_process'] = [train_loss, train_acc, val_loss, val_acc]\n",
        "\n",
        "    y_pred = np.argmax(my_model.predict(x_test), axis=1)\n",
        "    res['true_pred_lbls'] = [y_test, y_pred]\n",
        "\n",
        "    prec = precision(y_test, y_pred, average=None)\n",
        "    rec = recall(y_test, y_pred, average=None)\n",
        "    f = f1_score(y_test, y_pred, average=None)\n",
        "    res['p_r_f'] = [prec, rec, f]\n",
        "\n",
        "    acc = my_model.evaluate(x_test, y_test_cat)[1]\n",
        "    print(prec, '\\n', rec, '\\n', f)\n",
        "    res['acc'] = acc\n",
        "\n",
        "    perf_kfold[str(i)] = res\n",
        "\n",
        "    my_model.save(model_dir+model_name+'_'+'k'+str(i)+'.keras')\n",
        "  performance[model_name] = perf_kfold\n",
        "\n",
        "import joblib\n",
        "joblib.dump(performance, code_dir+'performance-'+model_name+'.joblib')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aX5ACBacyEud",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ff12ea7-28c5-4967-a30e-38fa42cf07e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vit fold 0 \n",
            " ##################################################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 14, 14\n",
            "  warnings.warn(\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " random_flip_2 (RandomFlip)  (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " random_rotation_2 (RandomR  (None, 224, 224, 3)       0         \n",
            " otation)                                                        \n",
            "                                                                 \n",
            " vit-b16 (Functional)        (None, 768)               85798656  \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 3)                 2307      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 85800963 (327.30 MB)\n",
            "Trainable params: 85800963 (327.30 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "411/411 [==============================] - 118s 196ms/step - loss: 0.4555 - accuracy: 0.8216 - val_loss: 0.2512 - val_accuracy: 0.9025\n",
            "Epoch 2/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.2366 - accuracy: 0.9089 - val_loss: 0.1514 - val_accuracy: 0.9403\n",
            "Epoch 3/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1822 - accuracy: 0.9278 - val_loss: 0.1746 - val_accuracy: 0.9269\n",
            "Epoch 4/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1949 - accuracy: 0.9272 - val_loss: 0.1581 - val_accuracy: 0.9385\n",
            "Epoch 5/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1791 - accuracy: 0.9362 - val_loss: 0.1405 - val_accuracy: 0.9470\n",
            "Epoch 6/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1767 - accuracy: 0.9351 - val_loss: 0.1259 - val_accuracy: 0.9519\n",
            "Epoch 7/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1563 - accuracy: 0.9416 - val_loss: 0.1210 - val_accuracy: 0.9549\n",
            "Epoch 8/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1457 - accuracy: 0.9488 - val_loss: 0.1333 - val_accuracy: 0.9519\n",
            "Epoch 9/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1809 - accuracy: 0.9320 - val_loss: 0.1171 - val_accuracy: 0.9640\n",
            "Epoch 10/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1604 - accuracy: 0.9409 - val_loss: 0.1191 - val_accuracy: 0.9531\n",
            "Epoch 11/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1434 - accuracy: 0.9464 - val_loss: 0.1820 - val_accuracy: 0.9342\n",
            "Epoch 12/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1453 - accuracy: 0.9456 - val_loss: 0.1203 - val_accuracy: 0.9561\n",
            "Epoch 13/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.2329 - accuracy: 0.9163 - val_loss: 0.3666 - val_accuracy: 0.8678\n",
            "Epoch 14/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.2856 - accuracy: 0.8917 - val_loss: 0.2890 - val_accuracy: 0.8995\n",
            "Epoch 15/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.2209 - accuracy: 0.9186 - val_loss: 0.1605 - val_accuracy: 0.9427\n",
            "Epoch 16/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1901 - accuracy: 0.9310 - val_loss: 0.1758 - val_accuracy: 0.9299\n",
            "Epoch 17/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1778 - accuracy: 0.9316 - val_loss: 0.1616 - val_accuracy: 0.9403\n",
            "Epoch 18/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1684 - accuracy: 0.9381 - val_loss: 0.1195 - val_accuracy: 0.9555\n",
            "Epoch 19/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1733 - accuracy: 0.9369 - val_loss: 0.1966 - val_accuracy: 0.9293\n",
            "Epoch 20/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1606 - accuracy: 0.9427 - val_loss: 0.1478 - val_accuracy: 0.9409\n",
            "Epoch 21/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1568 - accuracy: 0.9444 - val_loss: 0.1413 - val_accuracy: 0.9385\n",
            "Epoch 22/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1368 - accuracy: 0.9499 - val_loss: 0.1646 - val_accuracy: 0.9366\n",
            "Epoch 23/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1543 - accuracy: 0.9473 - val_loss: 0.1565 - val_accuracy: 0.9458\n",
            "Epoch 24/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1474 - accuracy: 0.9458 - val_loss: 0.1095 - val_accuracy: 0.9592\n",
            "Epoch 25/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1358 - accuracy: 0.9517 - val_loss: 0.1074 - val_accuracy: 0.9592\n",
            "Epoch 26/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1384 - accuracy: 0.9509 - val_loss: 0.1633 - val_accuracy: 0.9403\n",
            "Epoch 27/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1715 - accuracy: 0.9362 - val_loss: 0.1466 - val_accuracy: 0.9445\n",
            "Epoch 28/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1427 - accuracy: 0.9461 - val_loss: 0.1215 - val_accuracy: 0.9482\n",
            "Epoch 29/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1461 - accuracy: 0.9444 - val_loss: 0.1017 - val_accuracy: 0.9561\n",
            "Epoch 30/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1319 - accuracy: 0.9506 - val_loss: 0.0987 - val_accuracy: 0.9634\n",
            "Epoch 31/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1281 - accuracy: 0.9540 - val_loss: 0.1590 - val_accuracy: 0.9385\n",
            "Epoch 32/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1258 - accuracy: 0.9555 - val_loss: 0.1240 - val_accuracy: 0.9525\n",
            "Epoch 33/200\n",
            "411/411 [==============================] - 76s 186ms/step - loss: 0.1204 - accuracy: 0.9561 - val_loss: 0.0900 - val_accuracy: 0.9634\n",
            "Epoch 34/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1231 - accuracy: 0.9555 - val_loss: 0.0977 - val_accuracy: 0.9689\n",
            "Epoch 35/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1146 - accuracy: 0.9579 - val_loss: 0.1173 - val_accuracy: 0.9452\n",
            "Epoch 36/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1245 - accuracy: 0.9543 - val_loss: 0.0924 - val_accuracy: 0.9689\n",
            "Epoch 37/200\n",
            "411/411 [==============================] - 76s 186ms/step - loss: 0.1056 - accuracy: 0.9610 - val_loss: 0.0748 - val_accuracy: 0.9726\n",
            "Epoch 38/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1054 - accuracy: 0.9605 - val_loss: 0.0808 - val_accuracy: 0.9720\n",
            "Epoch 39/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1036 - accuracy: 0.9639 - val_loss: 0.1151 - val_accuracy: 0.9610\n",
            "Epoch 40/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1068 - accuracy: 0.9624 - val_loss: 0.1029 - val_accuracy: 0.9653\n",
            "Epoch 41/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.1026 - accuracy: 0.9645 - val_loss: 0.0783 - val_accuracy: 0.9677\n",
            "Epoch 42/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.0985 - accuracy: 0.9642 - val_loss: 0.1418 - val_accuracy: 0.9458\n",
            "Epoch 43/200\n",
            "411/411 [==============================] - 76s 186ms/step - loss: 0.0944 - accuracy: 0.9692 - val_loss: 0.0586 - val_accuracy: 0.9823\n",
            "Epoch 44/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.0880 - accuracy: 0.9695 - val_loss: 0.0909 - val_accuracy: 0.9634\n",
            "Epoch 45/200\n",
            "411/411 [==============================] - 76s 186ms/step - loss: 0.1084 - accuracy: 0.9627 - val_loss: 0.1309 - val_accuracy: 0.9500\n",
            "Epoch 46/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.0962 - accuracy: 0.9668 - val_loss: 0.0867 - val_accuracy: 0.9671\n",
            "Epoch 47/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.0831 - accuracy: 0.9714 - val_loss: 0.0613 - val_accuracy: 0.9793\n",
            "Epoch 48/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.0905 - accuracy: 0.9694 - val_loss: 0.1028 - val_accuracy: 0.9616\n",
            "Epoch 49/200\n",
            "411/411 [==============================] - 76s 186ms/step - loss: 0.1027 - accuracy: 0.9645 - val_loss: 0.0735 - val_accuracy: 0.9750\n",
            "Epoch 50/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.0961 - accuracy: 0.9680 - val_loss: 0.0708 - val_accuracy: 0.9756\n",
            "Epoch 51/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.0887 - accuracy: 0.9686 - val_loss: 0.0662 - val_accuracy: 0.9793\n",
            "Epoch 52/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.0881 - accuracy: 0.9701 - val_loss: 0.0673 - val_accuracy: 0.9775\n",
            "Epoch 53/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.0867 - accuracy: 0.9707 - val_loss: 0.1050 - val_accuracy: 0.9622\n",
            "Epoch 54/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.0876 - accuracy: 0.9718 - val_loss: 0.0795 - val_accuracy: 0.9720\n",
            "Epoch 55/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.0840 - accuracy: 0.9691 - val_loss: 0.0619 - val_accuracy: 0.9781\n",
            "Epoch 56/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.0841 - accuracy: 0.9709 - val_loss: 0.0907 - val_accuracy: 0.9628\n",
            "Epoch 57/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.0960 - accuracy: 0.9654 - val_loss: 0.0625 - val_accuracy: 0.9787\n",
            "Epoch 58/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.0815 - accuracy: 0.9715 - val_loss: 0.0627 - val_accuracy: 0.9805\n",
            "Epoch 59/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.0891 - accuracy: 0.9674 - val_loss: 0.0741 - val_accuracy: 0.9732\n",
            "Epoch 60/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.0903 - accuracy: 0.9674 - val_loss: 0.0652 - val_accuracy: 0.9793\n",
            "Epoch 61/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.0807 - accuracy: 0.9742 - val_loss: 0.0831 - val_accuracy: 0.9732\n",
            "Epoch 62/200\n",
            "411/411 [==============================] - 76s 185ms/step - loss: 0.0771 - accuracy: 0.9753 - val_loss: 0.0592 - val_accuracy: 0.9768\n",
            "Epoch 63/200\n",
            "411/411 [==============================] - 76s 186ms/step - loss: 0.1771 - accuracy: 0.9380 - val_loss: 0.2983 - val_accuracy: 0.8860\n",
            "10/10 [==============================] - 4s 112ms/step\n",
            "10/10 [==============================] - 1s 109ms/step - loss: 0.1822 - accuracy: 0.9519\n",
            "[0.97175141 0.94308943 0.75      ] \n",
            " [0.96089385 0.94308943 0.9       ] \n",
            " [0.96629213 0.94308943 0.81818182]\n",
            "vit fold 1 \n",
            " ##################################################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 14, 14\n",
            "  warnings.warn(\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " random_flip_3 (RandomFlip)  (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " random_rotation_3 (RandomR  (None, 224, 224, 3)       0         \n",
            " otation)                                                        \n",
            "                                                                 \n",
            " vit-b16 (Functional)        (None, 768)               85798656  \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3)                 2307      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 85800963 (327.30 MB)\n",
            "Trainable params: 85800963 (327.30 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "413/413 [==============================] - 118s 196ms/step - loss: 0.5141 - accuracy: 0.7913 - val_loss: 0.3370 - val_accuracy: 0.8814\n",
            "Epoch 2/200\n",
            "413/413 [==============================] - 77s 185ms/step - loss: 0.2652 - accuracy: 0.9040 - val_loss: 0.1856 - val_accuracy: 0.9243\n",
            "Epoch 3/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.2232 - accuracy: 0.9214 - val_loss: 0.1858 - val_accuracy: 0.9328\n",
            "Epoch 4/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.2098 - accuracy: 0.9273 - val_loss: 0.2284 - val_accuracy: 0.9195\n",
            "Epoch 5/200\n",
            "413/413 [==============================] - 77s 186ms/step - loss: 0.2035 - accuracy: 0.9278 - val_loss: 0.1637 - val_accuracy: 0.9383\n",
            "Epoch 6/200\n",
            "413/413 [==============================] - 77s 185ms/step - loss: 0.1883 - accuracy: 0.9337 - val_loss: 0.1625 - val_accuracy: 0.9346\n",
            "Epoch 7/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1947 - accuracy: 0.9302 - val_loss: 0.1888 - val_accuracy: 0.9389\n",
            "Epoch 8/200\n",
            "413/413 [==============================] - 77s 186ms/step - loss: 0.1742 - accuracy: 0.9385 - val_loss: 0.1361 - val_accuracy: 0.9455\n",
            "Epoch 9/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1621 - accuracy: 0.9417 - val_loss: 0.4076 - val_accuracy: 0.8880\n",
            "Epoch 10/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1731 - accuracy: 0.9364 - val_loss: 0.1453 - val_accuracy: 0.9377\n",
            "Epoch 11/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1688 - accuracy: 0.9385 - val_loss: 0.1497 - val_accuracy: 0.9455\n",
            "Epoch 12/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1544 - accuracy: 0.9456 - val_loss: 0.1525 - val_accuracy: 0.9449\n",
            "Epoch 13/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1572 - accuracy: 0.9479 - val_loss: 0.2106 - val_accuracy: 0.9370\n",
            "Epoch 14/200\n",
            "413/413 [==============================] - 77s 186ms/step - loss: 0.1700 - accuracy: 0.9423 - val_loss: 0.1209 - val_accuracy: 0.9570\n",
            "Epoch 15/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1677 - accuracy: 0.9423 - val_loss: 0.1465 - val_accuracy: 0.9431\n",
            "Epoch 16/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1473 - accuracy: 0.9476 - val_loss: 0.1362 - val_accuracy: 0.9510\n",
            "Epoch 17/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1474 - accuracy: 0.9461 - val_loss: 0.1711 - val_accuracy: 0.9383\n",
            "Epoch 18/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1438 - accuracy: 0.9475 - val_loss: 0.1324 - val_accuracy: 0.9498\n",
            "Epoch 19/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1362 - accuracy: 0.9518 - val_loss: 0.1478 - val_accuracy: 0.9540\n",
            "Epoch 20/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1453 - accuracy: 0.9484 - val_loss: 0.1254 - val_accuracy: 0.9546\n",
            "Epoch 21/200\n",
            "413/413 [==============================] - 77s 185ms/step - loss: 0.1399 - accuracy: 0.9505 - val_loss: 0.1379 - val_accuracy: 0.9437\n",
            "Epoch 22/200\n",
            "413/413 [==============================] - 77s 185ms/step - loss: 0.1196 - accuracy: 0.9576 - val_loss: 0.1188 - val_accuracy: 0.9546\n",
            "Epoch 23/200\n",
            "413/413 [==============================] - 77s 186ms/step - loss: 0.1401 - accuracy: 0.9499 - val_loss: 0.1069 - val_accuracy: 0.9588\n",
            "Epoch 24/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1204 - accuracy: 0.9555 - val_loss: 0.1281 - val_accuracy: 0.9546\n",
            "Epoch 25/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1237 - accuracy: 0.9556 - val_loss: 0.1169 - val_accuracy: 0.9522\n",
            "Epoch 26/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1599 - accuracy: 0.9470 - val_loss: 0.1277 - val_accuracy: 0.9504\n",
            "Epoch 27/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1692 - accuracy: 0.9422 - val_loss: 0.1297 - val_accuracy: 0.9552\n",
            "Epoch 28/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.2185 - accuracy: 0.9235 - val_loss: 0.1263 - val_accuracy: 0.9516\n",
            "Epoch 29/200\n",
            "413/413 [==============================] - 77s 185ms/step - loss: 0.1386 - accuracy: 0.9529 - val_loss: 0.1189 - val_accuracy: 0.9558\n",
            "Epoch 30/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1271 - accuracy: 0.9550 - val_loss: 0.1174 - val_accuracy: 0.9528\n",
            "Epoch 31/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1244 - accuracy: 0.9576 - val_loss: 0.1196 - val_accuracy: 0.9534\n",
            "Epoch 32/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1383 - accuracy: 0.9496 - val_loss: 0.1621 - val_accuracy: 0.9449\n",
            "Epoch 33/200\n",
            "413/413 [==============================] - 77s 186ms/step - loss: 0.1367 - accuracy: 0.9546 - val_loss: 0.0937 - val_accuracy: 0.9613\n",
            "Epoch 34/200\n",
            "413/413 [==============================] - 77s 186ms/step - loss: 0.1207 - accuracy: 0.9573 - val_loss: 0.0914 - val_accuracy: 0.9613\n",
            "Epoch 35/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1229 - accuracy: 0.9567 - val_loss: 0.0968 - val_accuracy: 0.9643\n",
            "Epoch 36/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1261 - accuracy: 0.9543 - val_loss: 0.1191 - val_accuracy: 0.9546\n",
            "Epoch 37/200\n",
            "413/413 [==============================] - 77s 185ms/step - loss: 0.1161 - accuracy: 0.9602 - val_loss: 0.0991 - val_accuracy: 0.9637\n",
            "Epoch 38/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1162 - accuracy: 0.9575 - val_loss: 0.0969 - val_accuracy: 0.9661\n",
            "Epoch 39/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1296 - accuracy: 0.9547 - val_loss: 0.1160 - val_accuracy: 0.9576\n",
            "Epoch 40/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1169 - accuracy: 0.9599 - val_loss: 0.1880 - val_accuracy: 0.9262\n",
            "Epoch 41/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1274 - accuracy: 0.9534 - val_loss: 0.1089 - val_accuracy: 0.9558\n",
            "Epoch 42/200\n",
            "413/413 [==============================] - 77s 185ms/step - loss: 0.1094 - accuracy: 0.9617 - val_loss: 0.0949 - val_accuracy: 0.9655\n",
            "Epoch 43/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1122 - accuracy: 0.9594 - val_loss: 0.1421 - val_accuracy: 0.9461\n",
            "Epoch 44/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1012 - accuracy: 0.9631 - val_loss: 0.1158 - val_accuracy: 0.9564\n",
            "Epoch 45/200\n",
            "413/413 [==============================] - 77s 186ms/step - loss: 0.1255 - accuracy: 0.9518 - val_loss: 0.0873 - val_accuracy: 0.9649\n",
            "Epoch 46/200\n",
            "413/413 [==============================] - 77s 186ms/step - loss: 0.1037 - accuracy: 0.9611 - val_loss: 0.0780 - val_accuracy: 0.9673\n",
            "Epoch 47/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1227 - accuracy: 0.9565 - val_loss: 0.1022 - val_accuracy: 0.9600\n",
            "Epoch 48/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1060 - accuracy: 0.9596 - val_loss: 0.1147 - val_accuracy: 0.9546\n",
            "Epoch 49/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1086 - accuracy: 0.9612 - val_loss: 0.1403 - val_accuracy: 0.9443\n",
            "Epoch 50/200\n",
            "413/413 [==============================] - 77s 185ms/step - loss: 0.1107 - accuracy: 0.9597 - val_loss: 0.0942 - val_accuracy: 0.9619\n",
            "Epoch 51/200\n",
            "413/413 [==============================] - 77s 186ms/step - loss: 0.0992 - accuracy: 0.9655 - val_loss: 0.0707 - val_accuracy: 0.9715\n",
            "Epoch 52/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1019 - accuracy: 0.9646 - val_loss: 0.2028 - val_accuracy: 0.9395\n",
            "Epoch 53/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1182 - accuracy: 0.9567 - val_loss: 0.1637 - val_accuracy: 0.9370\n",
            "Epoch 54/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1205 - accuracy: 0.9544 - val_loss: 0.0973 - val_accuracy: 0.9643\n",
            "Epoch 55/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1020 - accuracy: 0.9623 - val_loss: 0.0870 - val_accuracy: 0.9667\n",
            "Epoch 56/200\n",
            "413/413 [==============================] - 77s 185ms/step - loss: 0.1179 - accuracy: 0.9564 - val_loss: 0.1302 - val_accuracy: 0.9467\n",
            "Epoch 57/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1083 - accuracy: 0.9605 - val_loss: 0.0835 - val_accuracy: 0.9685\n",
            "Epoch 58/200\n",
            "413/413 [==============================] - 77s 185ms/step - loss: 0.0991 - accuracy: 0.9632 - val_loss: 0.1821 - val_accuracy: 0.9322\n",
            "Epoch 59/200\n",
            "413/413 [==============================] - 77s 185ms/step - loss: 0.1169 - accuracy: 0.9578 - val_loss: 0.1073 - val_accuracy: 0.9631\n",
            "Epoch 60/200\n",
            "413/413 [==============================] - 77s 185ms/step - loss: 0.0902 - accuracy: 0.9677 - val_loss: 0.1285 - val_accuracy: 0.9546\n",
            "Epoch 61/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1142 - accuracy: 0.9593 - val_loss: 0.1097 - val_accuracy: 0.9625\n",
            "Epoch 62/200\n",
            "413/413 [==============================] - 77s 185ms/step - loss: 0.1106 - accuracy: 0.9600 - val_loss: 0.0928 - val_accuracy: 0.9613\n",
            "Epoch 63/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.0955 - accuracy: 0.9627 - val_loss: 0.1180 - val_accuracy: 0.9492\n",
            "Epoch 64/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.0859 - accuracy: 0.9673 - val_loss: 0.1211 - val_accuracy: 0.9588\n",
            "Epoch 65/200\n",
            "413/413 [==============================] - 77s 185ms/step - loss: 0.1561 - accuracy: 0.9450 - val_loss: 0.1053 - val_accuracy: 0.9607\n",
            "Epoch 66/200\n",
            "413/413 [==============================] - 77s 185ms/step - loss: 0.1236 - accuracy: 0.9550 - val_loss: 0.0947 - val_accuracy: 0.9613\n",
            "Epoch 67/200\n",
            "413/413 [==============================] - 77s 185ms/step - loss: 0.1066 - accuracy: 0.9614 - val_loss: 0.0945 - val_accuracy: 0.9631\n",
            "Epoch 68/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1223 - accuracy: 0.9553 - val_loss: 0.1930 - val_accuracy: 0.9352\n",
            "Epoch 69/200\n",
            "413/413 [==============================] - 76s 185ms/step - loss: 0.1174 - accuracy: 0.9576 - val_loss: 0.1120 - val_accuracy: 0.9570\n",
            "Epoch 70/200\n",
            "413/413 [==============================] - 77s 185ms/step - loss: 0.1610 - accuracy: 0.9416 - val_loss: 0.1418 - val_accuracy: 0.9473\n",
            "Epoch 71/200\n",
            "413/413 [==============================] - 77s 186ms/step - loss: 0.1061 - accuracy: 0.9591 - val_loss: 0.1013 - val_accuracy: 0.9528\n",
            "10/10 [==============================] - 4s 107ms/step\n",
            "10/10 [==============================] - 1s 109ms/step - loss: 0.1937 - accuracy: 0.9327\n",
            "[0.94897959 0.92380952 0.72727273] \n",
            " [0.96875    0.88181818 0.8       ] \n",
            " [0.95876289 0.90232558 0.76190476]\n",
            "vit fold 2 \n",
            " ##################################################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 14, 14\n",
            "  warnings.warn(\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " random_flip_4 (RandomFlip)  (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " random_rotation_4 (RandomR  (None, 224, 224, 3)       0         \n",
            " otation)                                                        \n",
            "                                                                 \n",
            " vit-b16 (Functional)        (None, 768)               85798656  \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 3)                 2307      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 85800963 (327.30 MB)\n",
            "Trainable params: 85800963 (327.30 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "404/404 [==============================] - 114s 189ms/step - loss: 0.3928 - accuracy: 0.8715 - val_loss: 0.2162 - val_accuracy: 0.9127\n",
            "Epoch 2/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.2737 - accuracy: 0.9050 - val_loss: 0.2758 - val_accuracy: 0.9146\n",
            "Epoch 3/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.3373 - accuracy: 0.8741 - val_loss: 0.3013 - val_accuracy: 0.8923\n",
            "Epoch 4/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.2160 - accuracy: 0.9218 - val_loss: 0.2171 - val_accuracy: 0.9140\n",
            "Epoch 5/200\n",
            "404/404 [==============================] - 73s 182ms/step - loss: 0.2155 - accuracy: 0.9206 - val_loss: 0.1674 - val_accuracy: 0.9406\n",
            "Epoch 6/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.2032 - accuracy: 0.9241 - val_loss: 0.2340 - val_accuracy: 0.9109\n",
            "Epoch 7/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.1946 - accuracy: 0.9266 - val_loss: 0.2727 - val_accuracy: 0.8998\n",
            "Epoch 8/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.1971 - accuracy: 0.9285 - val_loss: 0.2930 - val_accuracy: 0.8731\n",
            "Epoch 9/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.1789 - accuracy: 0.9350 - val_loss: 0.2053 - val_accuracy: 0.9220\n",
            "Epoch 10/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.2056 - accuracy: 0.9231 - val_loss: 0.2882 - val_accuracy: 0.8967\n",
            "Epoch 11/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.1829 - accuracy: 0.9310 - val_loss: 0.2040 - val_accuracy: 0.9196\n",
            "Epoch 12/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.1851 - accuracy: 0.9314 - val_loss: 0.1990 - val_accuracy: 0.9233\n",
            "Epoch 13/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.1753 - accuracy: 0.9333 - val_loss: 0.2637 - val_accuracy: 0.9189\n",
            "Epoch 14/200\n",
            "404/404 [==============================] - 73s 182ms/step - loss: 0.2193 - accuracy: 0.9204 - val_loss: 0.1502 - val_accuracy: 0.9425\n",
            "Epoch 15/200\n",
            "404/404 [==============================] - 73s 182ms/step - loss: 0.1832 - accuracy: 0.9345 - val_loss: 0.1727 - val_accuracy: 0.9338\n",
            "Epoch 16/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.1818 - accuracy: 0.9310 - val_loss: 0.2377 - val_accuracy: 0.9134\n",
            "Epoch 17/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.1946 - accuracy: 0.9265 - val_loss: 0.1670 - val_accuracy: 0.9325\n",
            "Epoch 18/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.1648 - accuracy: 0.9387 - val_loss: 0.2055 - val_accuracy: 0.9251\n",
            "Epoch 19/200\n",
            "404/404 [==============================] - 73s 182ms/step - loss: 0.1810 - accuracy: 0.9324 - val_loss: 0.1583 - val_accuracy: 0.9369\n",
            "Epoch 20/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.1588 - accuracy: 0.9402 - val_loss: 0.1538 - val_accuracy: 0.9344\n",
            "Epoch 21/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.1979 - accuracy: 0.9245 - val_loss: 0.1907 - val_accuracy: 0.9313\n",
            "Epoch 22/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.1791 - accuracy: 0.9365 - val_loss: 0.1822 - val_accuracy: 0.9332\n",
            "Epoch 23/200\n",
            "404/404 [==============================] - 73s 182ms/step - loss: 0.1801 - accuracy: 0.9308 - val_loss: 0.1547 - val_accuracy: 0.9406\n",
            "Epoch 24/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.1902 - accuracy: 0.9259 - val_loss: 0.1825 - val_accuracy: 0.9270\n",
            "Epoch 25/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.1926 - accuracy: 0.9282 - val_loss: 0.1707 - val_accuracy: 0.9313\n",
            "Epoch 26/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.1652 - accuracy: 0.9382 - val_loss: 0.1664 - val_accuracy: 0.9344\n",
            "Epoch 27/200\n",
            "404/404 [==============================] - 73s 182ms/step - loss: 0.1914 - accuracy: 0.9266 - val_loss: 0.1746 - val_accuracy: 0.9338\n",
            "Epoch 28/200\n",
            "404/404 [==============================] - 73s 182ms/step - loss: 0.1922 - accuracy: 0.9288 - val_loss: 0.1747 - val_accuracy: 0.9344\n",
            "Epoch 29/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.2396 - accuracy: 0.9107 - val_loss: 0.2938 - val_accuracy: 0.8954\n",
            "Epoch 30/200\n",
            "404/404 [==============================] - 73s 182ms/step - loss: 0.3572 - accuracy: 0.8478 - val_loss: 0.4213 - val_accuracy: 0.8255\n",
            "Epoch 31/200\n",
            "404/404 [==============================] - 73s 181ms/step - loss: 0.3426 - accuracy: 0.8738 - val_loss: 0.3417 - val_accuracy: 0.8725\n",
            "Epoch 32/200\n",
            "404/404 [==============================] - 73s 182ms/step - loss: 0.2730 - accuracy: 0.9014 - val_loss: 0.2717 - val_accuracy: 0.8985\n",
            "Epoch 33/200\n",
            "404/404 [==============================] - 73s 182ms/step - loss: 0.2285 - accuracy: 0.9180 - val_loss: 0.1839 - val_accuracy: 0.9251\n",
            "Epoch 34/200\n",
            "404/404 [==============================] - 74s 182ms/step - loss: 0.2337 - accuracy: 0.9119 - val_loss: 0.2160 - val_accuracy: 0.9103\n",
            "10/10 [==============================] - 4s 105ms/step\n",
            "10/10 [==============================] - 4s 111ms/step - loss: 0.1385 - accuracy: 0.9519\n",
            "[0.96531792 0.95833333 0.78947368] \n",
            " [0.97093023 0.92       1.        ] \n",
            " [0.96811594 0.93877551 0.88235294]\n",
            "vit fold 3 \n",
            " ##################################################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 14, 14\n",
            "  warnings.warn(\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " random_flip_5 (RandomFlip)  (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " random_rotation_5 (RandomR  (None, 224, 224, 3)       0         \n",
            " otation)                                                        \n",
            "                                                                 \n",
            " vit-b16 (Functional)        (None, 768)               85798656  \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 3)                 2307      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 85800963 (327.30 MB)\n",
            "Trainable params: 85800963 (327.30 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "405/405 [==============================] - 117s 195ms/step - loss: 0.4306 - accuracy: 0.8390 - val_loss: 0.3536 - val_accuracy: 0.8802\n",
            "Epoch 2/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.2787 - accuracy: 0.8971 - val_loss: 0.1883 - val_accuracy: 0.9259\n",
            "Epoch 3/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.2243 - accuracy: 0.9187 - val_loss: 0.2545 - val_accuracy: 0.9117\n",
            "Epoch 4/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.2031 - accuracy: 0.9268 - val_loss: 0.2391 - val_accuracy: 0.9111\n",
            "Epoch 5/200\n",
            "405/405 [==============================] - 75s 186ms/step - loss: 0.1771 - accuracy: 0.9336 - val_loss: 0.1295 - val_accuracy: 0.9469\n",
            "Epoch 6/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1739 - accuracy: 0.9387 - val_loss: 0.1900 - val_accuracy: 0.9277\n",
            "Epoch 7/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1809 - accuracy: 0.9359 - val_loss: 0.1435 - val_accuracy: 0.9370\n",
            "Epoch 8/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1749 - accuracy: 0.9384 - val_loss: 0.1406 - val_accuracy: 0.9463\n",
            "Epoch 9/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1696 - accuracy: 0.9402 - val_loss: 0.2098 - val_accuracy: 0.9284\n",
            "Epoch 10/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1749 - accuracy: 0.9385 - val_loss: 0.1325 - val_accuracy: 0.9463\n",
            "Epoch 11/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1733 - accuracy: 0.9370 - val_loss: 0.1366 - val_accuracy: 0.9475\n",
            "Epoch 12/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1587 - accuracy: 0.9419 - val_loss: 0.1824 - val_accuracy: 0.9277\n",
            "Epoch 13/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.2112 - accuracy: 0.9263 - val_loss: 0.1534 - val_accuracy: 0.9370\n",
            "Epoch 14/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1655 - accuracy: 0.9413 - val_loss: 0.1871 - val_accuracy: 0.9222\n",
            "Epoch 15/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1988 - accuracy: 0.9252 - val_loss: 0.2236 - val_accuracy: 0.9172\n",
            "Epoch 16/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1778 - accuracy: 0.9322 - val_loss: 0.1665 - val_accuracy: 0.9376\n",
            "Epoch 17/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1604 - accuracy: 0.9418 - val_loss: 0.1334 - val_accuracy: 0.9469\n",
            "Epoch 18/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1630 - accuracy: 0.9401 - val_loss: 0.1295 - val_accuracy: 0.9469\n",
            "Epoch 19/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1568 - accuracy: 0.9430 - val_loss: 0.1298 - val_accuracy: 0.9512\n",
            "Epoch 20/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1509 - accuracy: 0.9445 - val_loss: 0.1300 - val_accuracy: 0.9555\n",
            "Epoch 21/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1468 - accuracy: 0.9472 - val_loss: 0.2291 - val_accuracy: 0.9135\n",
            "Epoch 22/200\n",
            "405/405 [==============================] - 75s 186ms/step - loss: 0.1514 - accuracy: 0.9433 - val_loss: 0.1260 - val_accuracy: 0.9537\n",
            "Epoch 23/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1525 - accuracy: 0.9442 - val_loss: 0.1365 - val_accuracy: 0.9456\n",
            "Epoch 24/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1524 - accuracy: 0.9424 - val_loss: 0.1422 - val_accuracy: 0.9450\n",
            "Epoch 25/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1534 - accuracy: 0.9427 - val_loss: 0.1342 - val_accuracy: 0.9463\n",
            "Epoch 26/200\n",
            "405/405 [==============================] - 75s 186ms/step - loss: 0.1468 - accuracy: 0.9489 - val_loss: 0.1144 - val_accuracy: 0.9592\n",
            "Epoch 27/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1404 - accuracy: 0.9495 - val_loss: 0.1613 - val_accuracy: 0.9426\n",
            "Epoch 28/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1604 - accuracy: 0.9421 - val_loss: 0.1666 - val_accuracy: 0.9438\n",
            "Epoch 29/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1723 - accuracy: 0.9351 - val_loss: 0.1330 - val_accuracy: 0.9518\n",
            "Epoch 30/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1571 - accuracy: 0.9408 - val_loss: 0.1353 - val_accuracy: 0.9500\n",
            "Epoch 31/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.2108 - accuracy: 0.9254 - val_loss: 0.2441 - val_accuracy: 0.9166\n",
            "Epoch 32/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.2340 - accuracy: 0.9173 - val_loss: 0.1460 - val_accuracy: 0.9382\n",
            "Epoch 33/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1857 - accuracy: 0.9364 - val_loss: 0.1946 - val_accuracy: 0.9351\n",
            "Epoch 34/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1719 - accuracy: 0.9388 - val_loss: 0.3627 - val_accuracy: 0.8196\n",
            "Epoch 35/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.2627 - accuracy: 0.9042 - val_loss: 0.2248 - val_accuracy: 0.9185\n",
            "Epoch 36/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1897 - accuracy: 0.9309 - val_loss: 0.1462 - val_accuracy: 0.9401\n",
            "Epoch 37/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1852 - accuracy: 0.9333 - val_loss: 0.1518 - val_accuracy: 0.9382\n",
            "Epoch 38/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.2746 - accuracy: 0.8917 - val_loss: 0.2836 - val_accuracy: 0.8962\n",
            "Epoch 39/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.2257 - accuracy: 0.9158 - val_loss: 0.1692 - val_accuracy: 0.9351\n",
            "Epoch 40/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1957 - accuracy: 0.9277 - val_loss: 0.1702 - val_accuracy: 0.9376\n",
            "Epoch 41/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.2049 - accuracy: 0.9249 - val_loss: 0.1901 - val_accuracy: 0.9290\n",
            "Epoch 42/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1694 - accuracy: 0.9337 - val_loss: 0.1402 - val_accuracy: 0.9463\n",
            "Epoch 43/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1708 - accuracy: 0.9390 - val_loss: 0.1443 - val_accuracy: 0.9426\n",
            "Epoch 44/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1489 - accuracy: 0.9442 - val_loss: 0.1268 - val_accuracy: 0.9463\n",
            "Epoch 45/200\n",
            "405/405 [==============================] - 75s 185ms/step - loss: 0.1721 - accuracy: 0.9357 - val_loss: 0.1688 - val_accuracy: 0.9314\n",
            "Epoch 46/200\n",
            "405/405 [==============================] - 75s 186ms/step - loss: 0.1652 - accuracy: 0.9385 - val_loss: 0.2648 - val_accuracy: 0.9006\n",
            "10/10 [==============================] - 4s 107ms/step\n",
            "10/10 [==============================] - 1s 110ms/step - loss: 0.1245 - accuracy: 0.9487\n",
            "[0.97894737 0.9009901  0.9047619 ] \n",
            " [0.94897959 0.93814433 1.        ] \n",
            " [0.96373057 0.91919192 0.95      ]\n",
            "vit fold 4 \n",
            " ##################################################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/vit_keras/utils.py:81: UserWarning: Resizing position embeddings from 24, 24 to 14, 14\n",
            "  warnings.warn(\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " random_flip_6 (RandomFlip)  (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " random_rotation_6 (RandomR  (None, 224, 224, 3)       0         \n",
            " otation)                                                        \n",
            "                                                                 \n",
            " vit-b16 (Functional)        (None, 768)               85798656  \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 3)                 2307      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 85800963 (327.30 MB)\n",
            "Trainable params: 85800963 (327.30 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "406/406 [==============================] - 117s 199ms/step - loss: 0.5496 - accuracy: 0.7916 - val_loss: 0.3713 - val_accuracy: 0.8681\n",
            "Epoch 2/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.2977 - accuracy: 0.8945 - val_loss: 0.2327 - val_accuracy: 0.9168\n",
            "Epoch 3/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.2487 - accuracy: 0.9115 - val_loss: 0.2327 - val_accuracy: 0.9069\n",
            "Epoch 4/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.2268 - accuracy: 0.9169 - val_loss: 0.1870 - val_accuracy: 0.9346\n",
            "Epoch 5/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.2097 - accuracy: 0.9258 - val_loss: 0.2363 - val_accuracy: 0.9192\n",
            "Epoch 6/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1972 - accuracy: 0.9309 - val_loss: 0.2324 - val_accuracy: 0.9266\n",
            "Epoch 7/200\n",
            "406/406 [==============================] - 75s 186ms/step - loss: 0.1734 - accuracy: 0.9374 - val_loss: 0.1868 - val_accuracy: 0.9346\n",
            "Epoch 8/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1903 - accuracy: 0.9340 - val_loss: 0.2526 - val_accuracy: 0.9020\n",
            "Epoch 9/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1635 - accuracy: 0.9400 - val_loss: 0.1467 - val_accuracy: 0.9464\n",
            "Epoch 10/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1656 - accuracy: 0.9411 - val_loss: 0.1487 - val_accuracy: 0.9439\n",
            "Epoch 11/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1681 - accuracy: 0.9423 - val_loss: 0.1477 - val_accuracy: 0.9433\n",
            "Epoch 12/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1426 - accuracy: 0.9510 - val_loss: 0.1075 - val_accuracy: 0.9599\n",
            "Epoch 13/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1654 - accuracy: 0.9419 - val_loss: 0.1243 - val_accuracy: 0.9538\n",
            "Epoch 14/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1582 - accuracy: 0.9451 - val_loss: 0.1140 - val_accuracy: 0.9599\n",
            "Epoch 15/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1357 - accuracy: 0.9508 - val_loss: 0.1110 - val_accuracy: 0.9618\n",
            "Epoch 16/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1340 - accuracy: 0.9542 - val_loss: 0.2402 - val_accuracy: 0.9088\n",
            "Epoch 17/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1405 - accuracy: 0.9525 - val_loss: 0.1127 - val_accuracy: 0.9649\n",
            "Epoch 18/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1464 - accuracy: 0.9476 - val_loss: 0.1391 - val_accuracy: 0.9482\n",
            "Epoch 19/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1380 - accuracy: 0.9545 - val_loss: 0.1501 - val_accuracy: 0.9476\n",
            "Epoch 20/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1285 - accuracy: 0.9527 - val_loss: 0.1307 - val_accuracy: 0.9531\n",
            "Epoch 21/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1283 - accuracy: 0.9570 - val_loss: 0.1066 - val_accuracy: 0.9605\n",
            "Epoch 22/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1287 - accuracy: 0.9519 - val_loss: 0.0968 - val_accuracy: 0.9753\n",
            "Epoch 23/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1226 - accuracy: 0.9596 - val_loss: 0.1027 - val_accuracy: 0.9686\n",
            "Epoch 24/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1222 - accuracy: 0.9584 - val_loss: 0.1212 - val_accuracy: 0.9568\n",
            "Epoch 25/200\n",
            "406/406 [==============================] - 75s 186ms/step - loss: 0.1263 - accuracy: 0.9599 - val_loss: 0.0949 - val_accuracy: 0.9655\n",
            "Epoch 26/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1081 - accuracy: 0.9627 - val_loss: 0.1126 - val_accuracy: 0.9581\n",
            "Epoch 27/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1030 - accuracy: 0.9638 - val_loss: 0.1364 - val_accuracy: 0.9655\n",
            "Epoch 28/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1307 - accuracy: 0.9544 - val_loss: 0.1244 - val_accuracy: 0.9593\n",
            "Epoch 29/200\n",
            "406/406 [==============================] - 75s 186ms/step - loss: 0.1016 - accuracy: 0.9656 - val_loss: 0.0654 - val_accuracy: 0.9778\n",
            "Epoch 30/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1142 - accuracy: 0.9625 - val_loss: 0.1227 - val_accuracy: 0.9618\n",
            "Epoch 31/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1017 - accuracy: 0.9650 - val_loss: 0.0862 - val_accuracy: 0.9686\n",
            "Epoch 32/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1057 - accuracy: 0.9665 - val_loss: 0.0845 - val_accuracy: 0.9655\n",
            "Epoch 33/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.0962 - accuracy: 0.9648 - val_loss: 0.1389 - val_accuracy: 0.9531\n",
            "Epoch 34/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.0919 - accuracy: 0.9692 - val_loss: 0.1211 - val_accuracy: 0.9618\n",
            "Epoch 35/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1064 - accuracy: 0.9628 - val_loss: 0.1022 - val_accuracy: 0.9642\n",
            "Epoch 36/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.0931 - accuracy: 0.9669 - val_loss: 0.0797 - val_accuracy: 0.9747\n",
            "Epoch 37/200\n",
            "406/406 [==============================] - 75s 186ms/step - loss: 0.0828 - accuracy: 0.9743 - val_loss: 0.0603 - val_accuracy: 0.9821\n",
            "Epoch 38/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.0793 - accuracy: 0.9736 - val_loss: 0.0957 - val_accuracy: 0.9686\n",
            "Epoch 39/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1044 - accuracy: 0.9613 - val_loss: 0.1021 - val_accuracy: 0.9618\n",
            "Epoch 40/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.0855 - accuracy: 0.9693 - val_loss: 0.0729 - val_accuracy: 0.9692\n",
            "Epoch 41/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.0903 - accuracy: 0.9701 - val_loss: 0.1265 - val_accuracy: 0.9531\n",
            "Epoch 42/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.0845 - accuracy: 0.9701 - val_loss: 0.0829 - val_accuracy: 0.9655\n",
            "Epoch 43/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.0986 - accuracy: 0.9667 - val_loss: 0.0928 - val_accuracy: 0.9673\n",
            "Epoch 44/200\n",
            "406/406 [==============================] - 75s 186ms/step - loss: 0.0909 - accuracy: 0.9676 - val_loss: 0.0580 - val_accuracy: 0.9797\n",
            "Epoch 45/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.0825 - accuracy: 0.9735 - val_loss: 0.0687 - val_accuracy: 0.9766\n",
            "Epoch 46/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.0752 - accuracy: 0.9755 - val_loss: 0.0672 - val_accuracy: 0.9772\n",
            "Epoch 47/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1297 - accuracy: 0.9528 - val_loss: 0.1063 - val_accuracy: 0.9624\n",
            "Epoch 48/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1014 - accuracy: 0.9647 - val_loss: 0.1006 - val_accuracy: 0.9729\n",
            "Epoch 49/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1057 - accuracy: 0.9653 - val_loss: 0.1030 - val_accuracy: 0.9655\n",
            "Epoch 50/200\n",
            "406/406 [==============================] - 75s 186ms/step - loss: 0.0900 - accuracy: 0.9687 - val_loss: 0.0757 - val_accuracy: 0.9772\n",
            "Epoch 51/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.0869 - accuracy: 0.9696 - val_loss: 0.0615 - val_accuracy: 0.9809\n",
            "Epoch 52/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.0782 - accuracy: 0.9738 - val_loss: 0.0833 - val_accuracy: 0.9673\n",
            "Epoch 53/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.1048 - accuracy: 0.9632 - val_loss: 0.0837 - val_accuracy: 0.9692\n",
            "Epoch 54/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.0893 - accuracy: 0.9690 - val_loss: 0.0791 - val_accuracy: 0.9735\n",
            "Epoch 55/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.0743 - accuracy: 0.9746 - val_loss: 0.0891 - val_accuracy: 0.9723\n",
            "Epoch 56/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.0768 - accuracy: 0.9729 - val_loss: 0.1104 - val_accuracy: 0.9710\n",
            "Epoch 57/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.0779 - accuracy: 0.9738 - val_loss: 0.0748 - val_accuracy: 0.9729\n",
            "Epoch 58/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.0815 - accuracy: 0.9722 - val_loss: 0.0816 - val_accuracy: 0.9741\n",
            "Epoch 59/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.0776 - accuracy: 0.9747 - val_loss: 0.0867 - val_accuracy: 0.9692\n",
            "Epoch 60/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.0752 - accuracy: 0.9758 - val_loss: 0.0721 - val_accuracy: 0.9747\n",
            "Epoch 61/200\n",
            "406/406 [==============================] - 75s 186ms/step - loss: 0.0734 - accuracy: 0.9755 - val_loss: 0.0681 - val_accuracy: 0.9760\n",
            "Epoch 62/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.0725 - accuracy: 0.9738 - val_loss: 0.0606 - val_accuracy: 0.9778\n",
            "Epoch 63/200\n",
            "406/406 [==============================] - 75s 185ms/step - loss: 0.0696 - accuracy: 0.9776 - val_loss: 0.0804 - val_accuracy: 0.9686\n",
            "Epoch 64/200\n",
            "406/406 [==============================] - 75s 186ms/step - loss: 0.0827 - accuracy: 0.9735 - val_loss: 0.1061 - val_accuracy: 0.9575\n",
            "10/10 [==============================] - 4s 115ms/step\n",
            "10/10 [==============================] - 1s 109ms/step - loss: 0.1794 - accuracy: 0.9367\n",
            "[0.94174757 0.92631579 0.93333333] \n",
            " [0.97979798 0.87128713 0.82352941] \n",
            " [0.96039604 0.89795918 0.875     ]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Fungal-Fungal-interaction/performanceViT.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import glob\n",
        "\n",
        "perf_paths = joblib.joblib(code_dir+'*.joblib')\n",
        "a = joblib.load(perf_path[0]).update(joblib.load(perf_path[2]))"
      ],
      "metadata": {
        "id": "P4Ag-_edRa4n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}